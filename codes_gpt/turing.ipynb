{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49689.77953141087, 40635, -9054.779531410873)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"covid_data.csv\")\n",
    "italy_data = df[df[\"location\"]==\"Italy\"]\n",
    "italy_data = italy_data.sort_values(\"date\", ascending = True)\n",
    "\n",
    "\n",
    "\n",
    "# Filtering the data for Italy between 2020-02-28 and 2020-03-20\n",
    "italy_specific_period = italy_data.copy()\n",
    "italy_specific_period = italy_specific_period[(italy_specific_period['date'] >= '2020-02-28') & (italy_specific_period['date'] <= '2020-03-20')]\n",
    "\n",
    "italy_specific_period[\"total_cases\"] = italy_specific_period[\"new_cases\"].cumsum()\n",
    "\n",
    "\n",
    "# Extracting the total cases and the corresponding dates\n",
    "dates = italy_specific_period['date']\n",
    "total_cases = italy_specific_period['total_cases']\n",
    "\n",
    "# Defining the exponential function to fit\n",
    "def exponential_func(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "# Converting dates to a numerical format (days since the start date)\n",
    "days_since_start = (pd.to_datetime(dates) - pd.to_datetime('2020-02-28')).dt.days\n",
    "\n",
    "# Fitting the exponential function to the data\n",
    "params, _ = curve_fit(exponential_func, days_since_start, total_cases, method=\"lm\")\n",
    "\n",
    "# Predicting total cases on 2020-03-20 using the fitted exponential function\n",
    "day_to_predict = (pd.to_datetime('2020-03-20') - pd.to_datetime('2020-02-28')).days+1\n",
    "predicted_cases = exponential_func(day_to_predict, *params)\n",
    "\n",
    "# Actual total cases on 2020-03-20\n",
    "actual_cases = italy_specific_period[italy_specific_period['date'] == '2020-03-20']['total_cases'].values[0]\n",
    "\n",
    "# Calculating the difference\n",
    "difference = actual_cases - predicted_cases\n",
    "predicted_cases, actual_cases, difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_covid_data = pd.read_csv(f\"covid_data.csv\")\n",
    "\n",
    "# Getting the latest data for each country to represent the current state\n",
    "latest_new_data = new_covid_data.groupby('location').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature description expense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Example dataset: Replace this with your actual dataset\n",
    "data = {\n",
    "    'TransactionID': [1, 2, 3, 4, 5],\n",
    "    'Description': [\n",
    "        \"Coffee shop\",\n",
    "        \"Supermarket grocery shopping\",\n",
    "        \"Online subscription service\",\n",
    "        \"Utility bill payment\",\n",
    "        \"Coffee with friends\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Preprocess and vectorize transaction descriptions\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Description'])\n",
    "\n",
    "# Normalize the feature vectors to ensure better convergence\n",
    "X_normalized = normalize(X, norm='l2')\n",
    "\n",
    "# Step 2: Perform K-means clustering\n",
    "# Note: Choose the number of clusters (n_clusters) based on your dataset and needs\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(X_normalized)\n",
    "\n",
    "# Step 3: Assign the cluster labels to each transaction\n",
    "df['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Step 4: Review the results\n",
    "print(\"Cluster centroids (TF-IDF vectors):\")\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "print(\"\\nTransactions with their assigned clusters:\")\n",
    "print(df[['TransactionID', 'Description', 'Cluster']])\n",
    "\n",
    "# Optional: Analyze the clusters to label them manually\n",
    "# This could involve reviewing the terms closest to each cluster's centroid\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for i in range(n_clusters):\n",
    "    print(f\"\\nTop terms in cluster {i}:\")\n",
    "    centroid = kmeans.cluster_centers_[i]\n",
    "    sorted_centroids = centroid.argsort()[::-1]\n",
    "    for j in sorted_centroids[:10]:  # Adjust the number of terms you want to see\n",
    "        print(f\" {feature_names[j]} (score: {centroid[j]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_studies",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
