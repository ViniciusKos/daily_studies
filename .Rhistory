#calculate kurtosis
kurtosis(samples, na.rm=TRUE)
ggplot(data.frame(sample = samples), aes(sample = samples)) +
stat_qq() +
stat_qq_line() +
ggtitle("QQ Plot of Penguin Body Mass") +
xlab("Theoretical Quantiles") +
ylab("Sample Quantiles")
shapiro_test_result <- shapiro.test(samples)
shapiro_test_result
# Assuming you've already loaded the palmerpenguins package and its data
library(palmerpenguins)
# Using the body mass of penguins for this example
# Remove NA values from the dataset
penguin_data <- na.omit(penguins$body_mass_g)
# Calculate mean and standard deviation
mean_val <- mean(penguin_data)
sd_val <- sd(penguin_data)
# Calculate percentages within one, two, and three standard deviations
within_one_sd <- mean(abs(penguin_data - mean_val) <= sd_val)
within_two_sd <- mean(abs(penguin_data - mean_val) <= 2 * sd_val)
within_three_sd <- mean(abs(penguin_data - mean_val) <= 3 * sd_val)
# Print results
cat("Percentage within 1 SD (expected ~68%):", within_one_sd * 100, "%\n")
cat("Percentage within 2 SD (expected ~95%):", within_two_sd * 100, "%\n")
cat("Percentage within 3 SD (expected ~99.7%):", within_three_sd * 100, "%\n")
# Assuming you've already loaded the palmerpenguins package and its data
library(palmerpenguins)
# Using the body mass of penguins for this example
# Remove NA values from the dataset
penguin_data <- na.omit(penguins$bill_length_mm)
# Calculate mean and standard deviation
mean_val <- mean(penguin_data)
sd_val <- sd(penguin_data)
# Calculate percentages within one, two, and three standard deviations
within_one_sd <- mean(abs(penguin_data - mean_val) <= sd_val)
within_two_sd <- mean(abs(penguin_data - mean_val) <= 2 * sd_val)
within_three_sd <- mean(abs(penguin_data - mean_val) <= 3 * sd_val)
# Print results
cat("Percentage within 1 SD (expected ~68%):", within_one_sd * 100, "%\n")
cat("Percentage within 2 SD (expected ~95%):", within_two_sd * 100, "%\n")
cat("Percentage within 3 SD (expected ~99.7%):", within_three_sd * 100, "%\n")
library(tidyverse)
library(ggplot2)
loan <- read_csv("data\cr_loan2.csv")
loan <- read_csv("data/cr_loan2.csv")
head(loan)
loan |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = loan_intent)
loan |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = loan_intent) |>
mutate(total = good+bad,
dist_good = good/sum(good))
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = feature) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total>0)
}
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = feature) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total>0)
}
calculate(loan, loan_intent, loan_status)
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = !!sym(feature)) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total>0)
}
calculate(loan, loan_intent, loan_status)
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = !!sym(feature)) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total>0)
}
calculate(loan, loan_intent, loan_status)
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = feature |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total>0)
}
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = feature |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)}
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = feature |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
}
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = feature) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
}
calculate(loan, loan_intent, loan_status)
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = !!sym(feature)) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
}
calculate(loan, loan_intent, loan_status)
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = loan_intent |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
}
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = loan_intent) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
}
calculate(loan, loan_intent, loan_status)
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = feature) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
}
calculate(loan, "loan_intent", "loan_status")
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(loan_status),
bad = sum(loan_status),
.by = loan_intent) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
}
woe_table <- calculate(loan, "loan_intent", "loan_status")
woe_table |> head()
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(target == 1),
bad = sum(target == 0),
.by = loan_intent) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
}
woe_table <- calculate(loan, "loan_intent", "loan_status")
woe_table |> head()
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(target == 1),
bad = sum(target == 0),
.by = loan_intent) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
}
woe_table <- calculate(loan, "loan_intent", "loan_status")
woe_table |> head()
loan |>
summarise(good = sum(loan_status==1),
bad = sum(loan_status==0),
.by = loan_intent) |>
mutate(total = good+bad,
dist_good = good/sum(good))
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(!!sym(target) == 1),
bad = sum(!!sym(target) == 0),
.by = loan_intent) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
}
woe_table <- calculate(loan, "loan_intent", "loan_status")
woe_table |> head()
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(!!sym(target) == 1),
bad = sum(!!sym(target) == 0),
.by = loan_intent) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
woe_table <- log(dist_bad/dist_good)
return(woe_table)
}
woe_table <- calculate(loan, "loan_intent", "loan_status")
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(!!sym(target) == 1),
bad = sum(!!sym(target) == 0),
.by = loan_intent) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
woe_table <- dist_table |>
mutate(woe = log(dist_bad/dist_good))
return(woe_table)
}
woe_table <- calculate(loan, "loan_intent", "loan_status")
woe_table |> head()
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(!!sym(target) == 1),
bad = sum(!!sym(target) == 0),
.by = loan_intent) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
woe_table <- dist_table |>
mutate(
proportion = dist_bad/dist_good,
woe = log(dist_bad/dist_good))
return(woe_table)
}
woe_table <- calculate(loan, "loan_intent", "loan_status")
woe_table |> head()
options(digits=2)
calculate <- function(data, feature, target) {
dist_table <- data |>
summarise(good = sum(!!sym(target) == 1),
bad = sum(!!sym(target) == 0),
.by = loan_intent) |>
mutate(total = good+bad,
dist_good = good/sum(good),
dist_bad = bad/sum(bad)) |>
filter(total > 0)
woe_table <- dist_table |>
mutate(
proportion = dist_bad/dist_good,
woe = log(dist_bad/dist_good))
return(woe_table)
}
woe_table <- calculate(loan, "loan_intent", "loan_status")
woe_table |> head()
install.packages("pwr")
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(pwr)
# Generate toy dataset
set.seed(123)  # for reproducibility
data1 <- rnorm(50, mean = 100, sd = 15)  # first sample
data2 <- rnorm(50, mean = 110, sd = 15)  # second sample
# Normality test - Shapiro-Wilk test
shapiro.test(data1)
shapiro.test(data2)
# Perform t-test
t_test_result <- t.test(data1, data2, var.equal = TRUE)  # assuming equal variance
# Output results
print(t_test_result)
# Effect size - Cohen's d
effect_size <- (mean(data1) - mean(data2)) / sqrt(((49 * var(data1)) + (49 * var(data2))) / 50)
print(paste("Effect size (Cohen's d):", effect_size))
# Test power calculation
pwr_result <- pwr.t.test(n = 50,
d = effect_size,
sig.level = t_test_result$p.value,
type = "two.sample",
alternative = "two.sided")
print(pwr_result)
# Confidence intervals
conf_int <- t_test_result$conf.int
print(paste("Confidence Intervals:", conf_int[1], "to", conf_int[2]))
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(pwr)
# Generate toy dataset
set.seed(123)  # for reproducibility
data1 <- rnorm(50, mean = 100, sd = 30)  # first sample
data2 <- rnorm(50, mean = 110, sd = 15)  # second sample
# Normality test - Shapiro-Wilk test
shapiro.test(data1)
shapiro.test(data2)
# Perform t-test
t_test_result <- t.test(data1, data2, var.equal = TRUE)  # assuming equal variance
# Output results
print(t_test_result)
# Effect size - Cohen's d
effect_size <- (mean(data1) - mean(data2)) / sqrt(((49 * var(data1)) + (49 * var(data2))) / 50)
print(paste("Effect size (Cohen's d):", effect_size))
# Test power calculation
pwr_result <- pwr.t.test(n = 50,
d = effect_size,
sig.level = t_test_result$p.value,
type = "two.sample",
alternative = "two.sided")
print(pwr_result)
# Confidence intervals
conf_int <- t_test_result$conf.int
print(paste("Confidence Intervals:", conf_int[1], "to", conf_int[2]))
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(pwr)
# Generate toy dataset
set.seed(123)  # for reproducibility
data1 <- rnorm(50, mean = 100, sd = 60)  # first sample
data2 <- rnorm(50, mean = 110, sd = 15)  # second sample
# Normality test - Shapiro-Wilk test
shapiro.test(data1)
shapiro.test(data2)
# Perform t-test
t_test_result <- t.test(data1, data2, var.equal = TRUE)  # assuming equal variance
# Output results
print(t_test_result)
# Effect size - Cohen's d
effect_size <- (mean(data1) - mean(data2)) / sqrt(((49 * var(data1)) + (49 * var(data2))) / 50)
print(paste("Effect size (Cohen's d):", effect_size))
# Test power calculation
pwr_result <- pwr.t.test(n = 50,
d = effect_size,
sig.level = t_test_result$p.value,
type = "two.sample",
alternative = "two.sided")
print(pwr_result)
# Confidence intervals
conf_int <- t_test_result$conf.int
print(paste("Confidence Intervals:", conf_int[1], "to", conf_int[2]))
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(pwr)
# Generate toy dataset
set.seed(123)  # for reproducibility
data1 <- rnorm(50, mean = 100, sd = 80)  # first sample
data2 <- rnorm(50, mean = 110, sd = 15)  # second sample
# Normality test - Shapiro-Wilk test
shapiro.test(data1)
shapiro.test(data2)
# Perform t-test
t_test_result <- t.test(data1, data2, var.equal = TRUE)  # assuming equal variance
# Output results
print(t_test_result)
# Effect size - Cohen's d
effect_size <- (mean(data1) - mean(data2)) / sqrt(((49 * var(data1)) + (49 * var(data2))) / 50)
print(paste("Effect size (Cohen's d):", effect_size))
# Test power calculation
pwr_result <- pwr.t.test(n = 50,
d = effect_size,
sig.level = t_test_result$p.value,
type = "two.sample",
alternative = "two.sided")
print(pwr_result)
# Confidence intervals
conf_int <- t_test_result$conf.int
print(paste("Confidence Intervals:", conf_int[1], "to", conf_int[2]))
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(pwr)
# Generate toy dataset
set.seed(123)  # for reproducibility
data1 <- rnorm(30, mean = 100, sd = 80)  # first sample
data2 <- rnorm(50, mean = 110, sd = 15)  # second sample
# Normality test - Shapiro-Wilk test
shapiro.test(data1)
shapiro.test(data2)
# Perform t-test
t_test_result <- t.test(data1, data2, var.equal = TRUE)  # assuming equal variance
# Output results
print(t_test_result)
# Effect size - Cohen's d
effect_size <- (mean(data1) - mean(data2)) / sqrt(((49 * var(data1)) + (49 * var(data2))) / 50)
print(paste("Effect size (Cohen's d):", effect_size))
# Test power calculation
pwr_result <- pwr.t.test(n = 50,
d = effect_size,
sig.level = t_test_result$p.value,
type = "two.sample",
alternative = "two.sided")
print(pwr_result)
# Confidence intervals
conf_int <- t_test_result$conf.int
print(paste("Confidence Intervals:", conf_int[1], "to", conf_int[2]))
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(pwr)
# Generate toy dataset
set.seed(123)  # for reproducibility
data1 <- rnorm(50, mean = 100, sd = 80)  # first sample
data2 <- rnorm(50, mean = 110, sd = 15)  # second sample
# Normality test - Shapiro-Wilk test
shapiro.test(data1)
shapiro.test(data2)
# Perform t-test
t_test_result <- t.test(data1, data2, var.equal = TRUE)  # assuming equal variance
# Output results
print(t_test_result)
# Effect size - Cohen's d
effect_size <- (mean(data1) - mean(data2)) / sqrt(((49 * var(data1)) + (49 * var(data2))) / 50)
print(paste("Effect size (Cohen's d):", effect_size))
# Test power calculation
pwr_result <- pwr.t.test(n = 50,
d = effect_size,
sig.level = t_test_result$p.value,
type = "two.sample",
alternative = "two.sided")
print(pwr_result)
# Confidence intervals
conf_int <- t_test_result$conf.int
print(paste("Confidence Intervals:", conf_int[1], "to", conf_int[2]))
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(pwr)
# Generate toy dataset
set.seed(123)  # for reproducibility
data1 <- rnorm(50, mean = 100, sd = 30)  # first sample
data2 <- rnorm(50, mean = 110, sd = 15)  # second sample
# Normality test - Shapiro-Wilk test
shapiro.test(data1)
shapiro.test(data2)
# Perform t-test
t_test_result <- t.test(data1, data2, var.equal = TRUE)  # assuming equal variance
# Output results
print(t_test_result)
# Effect size - Cohen's d
effect_size <- (mean(data1) - mean(data2)) / sqrt(((49 * var(data1)) + (49 * var(data2))) / 50)
print(paste("Effect size (Cohen's d):", effect_size))
# Test power calculation
pwr_result <- pwr.t.test(n = 50,
d = effect_size,
sig.level = t_test_result$p.value,
type = "two.sample",
alternative = "two.sided")
print(pwr_result)
# Confidence intervals
conf_int <- t_test_result$conf.int
print(paste("Confidence Intervals:", conf_int[1], "to", conf_int[2]))
t_test_result
# Function to calculate degrees of freedom for unequal variances t-test
welch_df <- function(n1, n2, var1, var2) {
df <- (var1/n1 + var2/n2)^2 / ((var1^2) / (n1^2 * (n1 - 1)) + (var2^2) / (n2^2 * (n2 - 1)))
return(df)
}
# Example usage:
# n1, n2 = sample sizes for the two groups
# var1, var2 = sample variances for the two groups
n1 <- 40  # Sample size of group 1
n2 <- 50  # Sample size of group 2
var1 <- 100  # Variance of group 1
var2 <- 144  # Variance of group 2
degrees_of_freedom <- welch_df(n1, n2, var1, var2)
print(degrees_of_freedom)
